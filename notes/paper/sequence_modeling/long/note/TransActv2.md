# TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation

# 标题
- 参考论文：TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation
- 公司：Pinterest
- 链接：https://arxiv.org/pdf/2506.02267
- Code：暂时没找到
- 时间：2025
- `精读`

# 内容

## 摘要
- 问题：
  - 序列过短：只能捕捉短期兴趣，缺乏对用户长期行为模式的建模能力。
  - 任务单一：CTR预测模型未与用户下一步行为预测任务进行联合优化，限制了模型对用户意图的理解深度。
  - 工程缺失：大多研究忽视将超长序列模型低成本、低延迟部署的工程挑战。
- 方法：
  - 超长序列建模：直接处理非常长的用户行为甚至是终生的序列，以全面捕捉长短期兴趣。
  - 辅助任务学习：在排序框架中引入 “下一个行动预测” 作为辅助损失，使主CTR任务与更深层的用户行为建模相互增强。
  - 系统工程：设计了配套的数据处理与服务优化方案，确保高复杂度的模型能高效上线。
  - **本质上和V1一样，但是增加了长序列的长度，并且实现了实时。同时增加了一个辅助任务。辅助这里参考了DIEN的思路**

## 1 Introduction
- 问题：
  - 序列长度受限：因成本与延迟，工业模型多处理短期序列（约100条），无法建模完整的用户生命周期行为（可能长达万条）。
  - 建模目标单一：主流CTR模型仅预测“点击”，未与更深层的用户行为预测（如下一步动作）联合优化，限制了理解深度。
  - 工程方案低效：处理长序列的现有方案（如压缩、离线缓存）存在信息损失或成本高昂的问题。
- 方法：
  - 终身序列建模：直接端到端处理超长用户行为序列（10⁴量级），全面捕捉长短期兴趣。
  - 引入下一个动作预测任务：在CTR模型中新增一个辅助学习任务，通过预测用户的下一步行为来增强主任务（CTR预测）的准确性和对用户意图的理解。
  - 可扩展的低成本部署：配套开发了一整套系统工程方案，实现了在巨大流量下，对超高复杂度模型的低延迟、高效率服务。
- **主要贡献**：
  - 介绍了 TransAct V2，这是一个结合实时和终身用户序列以提升用户参与度和推荐多样性的最先进模型。
  - 实现了一种新颖的下一个动作损失，使CTR模型能够更好地预测用户行为，从而提高预测准确性。
  - 为CTR模型开发了网络级服务解决方案，确保了低延迟性能和高资源效率，并辅以全面的消融研究，突出了每项优化对整体效率的贡献。

## 2 Related Work

### 2.1 Sequential Recommendation
- 序列模型的两大模式：
  - 下一项预测：经典序列推荐任务，直接预测用户接下来会交互什么。
  - CTR框架内的序列建模：将用户序列作为特征，输入到一个以预测点击率为目标的模型（如 point-wise Ranking）中。这是工业界排序阶段更主流的做法。
- 两阶段建模及其局限：
  - 常见思路：先检索（从超长历史中找出与当前候选相关的子集）也就是召回先，再编码（对子集应用Transformer）也就是排序 ranking。例如SIM、UBR4CTR、TransAct、TWIN
  - 问题：
    - 模型精度和延迟 trade off：
      - 限制检索后序列的长度（如TransAct V1的100条），则损失长期信息；若想利用终身序列，则需依赖高成本的离线预处理与缓存（如TWIN系列）。
    - 信息损失：
      - 压缩（如聚类）长期信息作为妥协方案，会导致信息丢失。（如TWIN V2系列）
    - 任务单一性：
      - 这些CTR模型缺乏对用户行为序列本身内在规律（如下一动作预测）的显式建模，这可能限制了其深度理解用户意图的能力。

### 2.2 Efficient Sequential Model Serving
- 核心：
  - 推荐系统通常是具有严格延迟约束的高吞吐量系统，实时处理长用户行为序列是非常困难的
- 现有方案的分类与局限：
  - 离线压缩（如TWIN v2, Trinity）：
    - 做法：离线将长序列压缩（如聚类）为简短摘要。
    - 问题：压缩过程是静态的、通用的，忽略了当前候选物品，因此损失了为特定候选进行精细化匹配的能力，信息利用不充分。
  - 独立模型压缩（如TWIN）：
    - 做法：用一个独立模型在线压缩序列。
    - 问题：引入了额外的模型维护和实时更新的复杂性。
  - 生成式架构（如HSTU）：
    - 做法：使用超大生成模型（万亿参数）处理序列。
    - 问题：计算成本极其高昂，难以满足工业场景的成本效益要求。

## 3 Methodology

### 3.1 Preliminary: Ranking Model
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_3_模型结构.png">
      <figcaption style="text-align: center">
        TransActv2_模型结构
      </figcaption>
    </img>
  </p>

- 模型架构
  - point-wise 排序模型
  - Wide & Deep 架构
  - 多任务学习模型，同时预测多种用户行为（即多个“头”）。
- loss 设计：
  - 加权交叉熵损失
  - 对不同任务，不同样本的权重不一样，和TransAct V1 一样
- 创新：
  - 在基础的多头预测之上，明确提出了引入 “下一个动作预测” 作为辅助任务。让模型学习用户行为的内在序列规律。

### 3.2 TransAct V2: Transformer for Lifelong User Action Sequence
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_3.2_TransAct结构.png">
      <figcaption style="text-align: center">
        TransActv2结构
      </figcaption>
    </img>
  </p>

- 长短期融合结构
- 短期实时序列捕捉短期用户行为，关注其最新的兴趣
- 终身用户序列，模型旨在平衡用户的即时偏好和历史模式，从而培养更丰富、更多样化的交互生态系统。提高了内容多样性，还增加了持续的用户参与度。

#### 3.2.1.Lifelong User Sequence Features
- 超长终身序列构建的核心原则：
  - 长期性：覆盖两年跨度，两年行为历史长度的第90百分位数。
  - 高质量信号：只纳入有明确意图的显式反馈，显式行为，如转存、点击、隐藏，排除了仅曝光的数据，从源头提升数据信噪比。
  - 面向核心用户：序列长度根据高频活跃用户的行为按访问频率加权，确保模型能充分学习核心用户群体的模式。
- 特征工程：
  - 特征构成：每个行为记录为一个包含行为时间戳、行为类型（如果与同一Pin有多次交互，则为多热向量）、行为发生界面（如首页、搜索）以及32维PinSage pretrained embedding。
  - 细粒度行为：行为类型采用多热向量，能精准记录用户对同一物品的复杂、多次互动（如“查看详情后又转存”），这是捕捉深度兴趣的关键。
- 存储与计算优化：
  - 问题：PinSage embedding 是主要存储开销。32维度。
  - 方案：采用仿射量（affine quantization）化进行有损压缩，将浮点数转换为整型，存储直接减半。
  - 公式：𝑞 = clamp( 𝑒 / 0.65 × 127, −127, 127 ) → int8
  - 量化公式中的缩放因子（0.65）是基于数据分布精心选择的，旨在最小化精度损失。

#### 3.2.2 Modeling Lifelong User Sequence
- 思路：检索 + 拼接
  - 目标：将处理万级 𝑂 (10^4) 的终身序列，压缩为处理百级 𝑂 (10^2) 的相关子序列。
  - 方法：以候选物品为查询依据，分别从终身序列、实时序列（剔除最近r条后）、曝光序列中，检索出与之最相似的K个历史行为。
  - 保障实时性：无论相似度如何，强制保留最近r条行为，确保模型对用户最新意图的响应灵敏度不被过滤掉。
- NN search 机制：
  - 基于物品内容嵌入（PinSage）的向量相似度（点积）进行检索
- 最终输出：
  - 公式：𝑺𝑎𝑙𝑙 = NN(𝑺𝐿𝐿, 𝑐) ⊕ 𝑺𝑅𝑇 [: 𝑟 ] ⊕ NN(𝑺𝑅𝑇 [𝑟 :], 𝑐) ⊕ NN(𝑺𝑖𝑚𝑝 , 𝑐)
  - 最终长度：several hundred elements 𝑂 (10^2)，后续实验显示是 192
  - 包含信息：长期相关兴趣、短期强制记忆、实时相关兴趣及曝光历史
- Feature Encoding：
  - 多源特征 embedding：
    - 行为语义：通过 E_act 编码“用户做了什么”（点击、收藏等）。
    - 场景上下文：通过 E_surf 编码“在哪做的”（首页、搜索页等）。
    - 时序/顺序信息：通过可学习的位置编码 E_pos 来感知顺序，而非使用固定的正弦余弦编码，使模型能更灵活地学习顺序模式。
  - 早期融合策略：
    - 在每个历史行为中，都显式拼接了候选物品的嵌入 e_c，使模型在编码序列的每一步都能直接感知当前待排序的物品，为后续的交互建模提供最直接的基础。这里和V1思路一模一样。
  - 融合拼接：
    - 思路：通过将所有辅助特征的维度 d_act、d_surf、d_pos 都设置为 2 * d_PinSage，使得拼接了候选物品信息的物品内容向量 CONCAT(E_PinSage(S_all), e_c) 与各项辅助特征的 embedding 维度完全一致。
    - 优势：这使得最终的表征可以通过简单的相加来融合所有信息，而非更常见的拼接。这极大减少了后续Transformer层的参数量和计算量，同时确保了不同特征间的交互能够直接发生。
    - **本质上把V1的 concat 思路变成了相加思路，引入了更多的辅助数据比如 E_surf。这样固定住了每个行为的输出维度是 32 * 2 维度。**
- Transformer Encoder：
  - Transformer设计：
    - 极度轻量：采用 2层、1头、自注意力64维、FFN 32维 的极小配置。这里比V1更轻了。
    - 因果掩码：使用因果掩码确保序列建模是自回归式的，即每个位置只能关注它之前的位置，这强制模型学习序列中符合时间逻辑的依赖关系。
  - 双任务输出接口：
    - 主任务路径：编码器输出的序列表征 U，经过一个 “线性层 + 最大池化” 的简单聚合，被压缩成一个固定长度的用户兴趣向量。这个向量作为高级特征，被送入排序主模型（Pinnability）的特征交叉层，与其他特征一同进行最终的多目标预测。
    - 辅助任务路径：U 同时也会被用于 “下一动作预测” 这个辅助任务，以提升主任务的学习效果。
    - **这里的压缩输出和V1有点不一样，但是思路一样，还是聚合一下长期的较远的行为，近行为保留但是多加了一个MLP。**

## 3.3 Next Action Loss
- 设计了一个辅助 loss，帮助 transformer 去预测下一个用户行为
- 辅助任务与多头预测一起显著提高了排序性能

### 3.3.1.Next Action Prediction Task
- 任务定义：
  - 任务：一个自监督的序列预测任务。给定用户到时间 *t* 为止的行为历史，要求模型预测其下一个互动物品（*t+1*时刻）的参与度是消极还是积极。
  - 目的：并非直接用于推荐，而是作为一个辅助训练信号，迫使Transformer编码器学习生成更能体现用户即时意图和兴趣演变趋势的高质量序列表征。
- 实现机制（对比学习）：
  - 正样本：用户实际在 *t+1* 时刻交互的物品的 pu(t + 1) PinSage embeddings。
  - 负样本：从整个物品池中采样得到的、用户未看到的物品。用 nu 表示这个物品的集合。
  - 损失函数：采用采样Softmax损失。用户当前时刻的表征 u(t) 与其下一步真实互动物品的表征在向量空间中的相似度（内积）尽可能高，同时与随机负样本的相似度尽可能低。
  - 这里的 u(t) 是经过 Transformer 𝑼 输出中的第 t 列表示为 u(t)
  - **本质上就是用户行为对正向候选item的内积越大越好，负向越小越好，这里用户行为是在提取了序列信息之后，也就是类似召回里面的双塔设计，对比DIEN的区别就是，用户行为 encode 后的信息包含了更多的行为层次的信息而不只是item 2 item 的预测，另外损失函数的设计不一样，DIEN采用CE，这里是softmax，更倾向于对比学习**
- 与主任务的融合：
  - 多任务学习框架：下一动作损失 L_NAL 与主CTR/CVR等多头预测损失 L_CE 加权求和，共同指导模型训练。
  - 协同效应：L_NAL 引导模型学习用户行为的动态模式；L_CE 引导模型优化最终的排序目标。

### 3.3.2 Key Modeling Design of NAL
- 因果掩码：
  - 作用：在Transformer编码器中强制实施自回归特性，确保在预测 t 时刻动作时，模型仅能基于 t 时刻之前的历史。这是进行合理序列预测的前提，避免了“未来信息泄露”。
  - 价值：这使模型学习的用户嵌入 u(t) 是纯粹基于过去行为的，从而让下一动作预测任务能够真实地模拟和强化模型对用户兴趣演变趋势的建模能力。
- 负采样策略：
  - 策略对比：
    - 批内随机负采样：简单但粗糙，这里的对于用户 u_i 的负样本来自同一个batch内 u_j 用户的序列采样 N 个，这里有个强假设，不同用户兴趣不同，会带来噪声较大。
    - 基于曝光的负采样：更精细、更符合业务逻辑。它选取的是用户真正见过但并未参与任何行为的物品，这些是“真实的负反馈” 比随机负样本更能挑战模型，使其学习到更细微的用户偏好区分能力。后续实验证明这个采样效果更好。
- 正样本策略：
  - 对于所有 𝑺𝑅𝑇 [: 𝑟 ] 中表现出积极参与的类型
- 损失函数的选择：
  - 对比学习的softmax loss 更灵活的调整正负样本的比例，同时学习到的是相似的pair 更近，不相似的pair 更远，对负样本的惩罚更多
  - CE 是常用的二元分类的 loss

## 3.4 Serving and Logging System Design
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_3.4_在线系统设计.png">
      <figcaption style="text-align: center">
        在线 serving 系统设计
      </figcaption>
    </img>
  </p>

- 挑战：
  - 令 L 表示 SLL （长序列）的长度， N 表示每个排序请求的平均项目数，L 的规模这里为 𝒪(10^4)
  - 存储成本：与序列长度 L 成线性关系 𝒪(L)
  - 网络成本：与请求候选数量 N 和 序列长度 L 的乘积成线性关系 𝒪(N * L)
- 方法：
  - 提出了一种新的机器学习数据管道，专门用于超长用户序列模型
  - 确保系统能够处理明显更长的用户序列，而不会影响延迟、存储效率或模型性能。

### 3.4.1 Data Pipelines
- 问题：
  - 特征通常在服务时存储于缓存中，但是终身 LL 特征（O(L)）太大无法高效缓存和进行网络传输。
- 方法：
  - 从记录“所有数据”转变为只记录对模型训练真正有用的“精华信息”——即针对每个训练样本中候选物品检索得到的NN的特征。
- 训练 vs. 服务：
  - 训练（离线、高效）：预先计算并存储NN特征。训练时直接加载这些精简、高质量的特征，避免了在线检索的巨大开销。
  - 服务（在线、实时）：必须实时响应。因此采取 “广播序列 + 实时NN搜索” 的策略：
    - 先将完整用户序列发送到计算设备，再为每个候选物品实时计算NN特征。这虽然增加了单次请求的计算量，但彻底避免了O(NL)的网络传输，是延迟与计算之间的重要权衡。
- 三大特点：
  - 存储效率：只记录NN特征而非完整序列，训练数据量从“万级”降至“百级”，实现本质性降低。最小化了存储和带宽成本。
  - 训练-服务对齐：训练和服务阶段输入给Transformer的特征格式完全一致（都是NN特征），确保了模型效果从离线到在线的无损迁移。
  - 可扩展推理：将耗时的NN搜索置于设备端，并通过优化使其高效，满足了工业级低延迟要求。

### 3.4.2 Serving Optimizations
- 问题：
  - 降低高QPS系统中服务大型语言模型用户序列而不会产生巨大的成本
- 方法：
  - 优化：
    - 已有：延续了TransAct V1中的成熟实践，即使用 CUDAGraph 来消除内核启动开销，request 被动态地批处理成小批量，并采用精细化的内存管理策略（可分页CPU内存 -> 固定内存 -> 静态GPU内存）来优化数据流。
    - 新式：引入了 Triton 框架。允许团队在更底层编写高度定制化的GPU内核，而不再受限于通用算子库（如PyTorch）的限制。
  - Triton的优势：
    - 精细控制：能够手动控制 Tensor 如何被切分以及如何利用GPU L2高速缓存，这对于处理不规则、超长的序列数据至关重要，能最大化缓存命中率，减少对慢速全局内存的访问。
    - 深度算子融合：解决 O(NL) 大量的CPU到GPU和GPU内部的数据传输。将原本需要多次内核调用和中间结果读写的复杂计算步骤，融合成一个或少数几个高度优化的定制内核。这极大地减少了：
      - 内核启动次数。
      - 中间数据在内存中的搬运。
- Request Level De-duplication 请求去重
  - 问题：
    - 在一个排序设置中，单个推理请求会对N个项目进行排名，用户序列的请求会被复制到 CPU 上的每个项目，然后传输到 GPU 上进行推理。
  - 方法:
    - 引入新型sparse tensor format，不需要在复制到每个 CPU 每个 item 上面，也可以处理用户序列特征
    - 开发一个自定义 Triton 内核，可以直接对去重的请求进行 NN 搜索（无复制 NN 搜索），从而无需在 GPU 上复制请求级别的序列特征
  - 结果：
    - 序列特征数据传输减少了 8 倍
- Fused Sequence Dequantization
  - 问题：
    - PinSage 序列特征以量化的 int8 格式存储。 在 NN 搜索之前，必须将此张量反量化到float16并进行归一化。然而 L2 归一化通常涉及启动四个单独的内核：平方、求和、钳位和除法。每个内核都会处理这个相当大的张量
  - 方法：
    - 将这个步骤和 NN 搜索内核结合起来，而不是在原生 pytorch 里面
  - 结果：
    - 模型延迟减少20%
- Single Kernel Unified Transformer (SKUT)
  - 效果：
    - 模型的低维度(d_model = 64)，我们开发了一种新颖的融合Transformer，其前向传递性能比PyTorch快6.6倍。
  - 方法：
    - 单个合并的自定义Triton内核：因为模型维度仅为64，可以将Transformer层的所有子操作（投影、注意力、归一化、前馈）编译成一个内核，彻底消除了子操作间的内核启动和内存读写传输开销。
- Pinned Memory Arena
  - 问题：
    - 在推理过程中，每个大小为128的小批量会产生64 MB的有效负载。此大型有效负载引入了两个复制瓶颈
      - 可分页内存到固定内存的复制
      - 以及固定CPU内存到GPU的复制
    - 后者已经被请求级别重复数据删除得到充分解决，前者会带来内存固定成本增加
  - 方法：
    - 实现了固定内存支持内存区域，并直接在其上构建整理的mini-batches
  - 结果：
    - 消除了可分页内存到固定内存的复制步骤，并将最终实验设置的推理速度提高了最多35%

<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_3.4_服务优化.png">
      <figcaption style="text-align: center">
        服务优化
      </figcaption>
    </img>
  </p>

## 4 Experiment
- 线上和线下实验对比，使用 Pinterest 内部自己的数据

### 4.1 Experiment Setup

#### 4.1.1 Dataset
- 2 周的数据 train，最后一周的数据 test
- 6.9 B train instance，182 M user 和 350 M pins
- 对比 V1，训练instance变多了，用户变多了，但是 item 变少了

### 4.2 Offline Experiment

#### 4.2.1.Metrics
- HIT@3
- 和 V1 一样，最终的输出 score 是每个 head 的加权平均

#### 4.2.2.Results
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_4.2_模型实验对比.png">
      <figcaption style="text-align: center">
        模型实验对比
      </figcaption>
    </img>
  </p>

- 主要对比 BST 和 TransAct v1，basemodel 可能是 WDL no sequence
- V2 的效果明显很好，其中基于印象的负采样（NALimp）对这一改进至关重要，模型受益于对用户不感兴趣的更细致的理解，从而更好地校准排序模型

### 4.3 Online Experiment
- 把 offline的 model 后面第二个，第四个，第五个都deploy了，每组服务 1.5% 首页访问用户
- 用 TransAct (RT sequence) 作为 base，保证了用户体验同时保证了对比的效果有意义

#### 4.3.1.Metrics
- Homefeed Repin Volume (higher is better, correlating with HIT@3/repin offline)
- Homefeed Hide Volume (lower is better, indicating irrelevant recommendations and poor user experience)
- Impression Diversity (higher is better, reflecting broader content exposure)

#### 4.3.2.Online Results
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_4.3_线上实验对比.png">
      <figcaption style="text-align: center">
        线上实验对比
      </figcaption>
    </img>
  </p>

- 加入了曝光的负采样能有效过滤不相关推荐
- 与 NALimp 集成的TransAct V2实现了最佳整体性能

### 4.4 Ablation Study
- 评估了辅助任务和Transformer的效果
- 测试了不同部分对于serving的优化

#### 4.4.1.Next Action Loss Ablation
<p style="text-align: center">
    <img src="../../pics/TransActv2/TransActv2_4.4_负样本采样方式对比.png">
      <figcaption style="text-align: center">
        负样本采样方式对比
      </figcaption>
    </img>
  </p>

- 明显 Impression-based negative sampling 效果最好，因为提供了更强大、更具挑战性的负面样本，从而提高了模型的泛化能力，同时提高了预测负面结果和减少隐藏信息的准确性。

#### 4.4.2.Transformer Hyperparameters
- S_all 192，2 层 layers 和 FNN 32 最好的平衡了latency 和 repin metrics
- 虽然加序列长度和前馈维度可以提高 HIT@3/重推率，但会显著增加推理延迟

#### 4.4.3.Serving Optimization Ablation
- Single Kernel Unified Transformer
  - Batch size 256, Sequence length 192
  - SKUT 实现了85.09%的延迟降低和13.24%的GPU内存占用减少
  - 主要来自于避免了Q、K、V张量的实例化 materialization
- Server Optimizations
  - CPU到GPU的复制时间的影响：
    - 批量大小为128时复制时间改善了85%，批量大小为256时改善了82%，批量大小为512时改善了85%。
  - 模型运行延迟包括CPU到GPU的特征复制、模型前向传递以及GPU到CPU的输出复制
    - 仅启用固定内存区域，并不能降低太多，因为受到PCIe带宽限制
    - 仅请求去重在所有延迟和批次大小下都显示出一致的降低
    - 把上述两种方式融合起来效果最好，批次大小为128、256、512时分别降低了75%、75%和81%。

## 5 Conclusions
- 优化了V1变成V2，加入了LL用户序列的TransAct V2 模型
- 同时加入了一个辅助任务，next action loss，引入了负样本
- 大量的工程上的优化，并且线上测试显示效果大幅提升同时解决延迟和存储挑战的能力



# 思考

## 本篇论文核心是讲了个啥东西
- 同样是提出了长期和短期的融合模型，和V1的思路一样
- 终身序列建模算法：提出一个统一的Transformer架构，通过面向候选物品的动态NN Search，从海量终身序列中实时筛选出数百个最相关的历史行为构成输入，从而将计算复杂度从万级降至百级。
- 下一动作预测辅助任务：在CTR预测主任务外，引入一个基于对比学习的自监督任务，要求模型预测用户的下一步行为。该任务迫使模型学习用户行为的动态演化规律，从而生成质量更高、更具时序意识的用户表征，显著提升了主任务效果和推荐多样性。并且学习了负样本的区别，这里的负样本是 Impression 但是没有任何行为的 item。
- 系统工程与高效推理：配套设计了一整套数据流水线和推理优化方案，详细参考 3.4 章节
- **本质上和V1一样，长短期都考虑，但是这里长期直接采用了 LL sequence，并且所有 sequence 都采用了 NN search先，只有最近的 r 个行为保留了全部sequence**

## 是为啥会提出这么个东西，为了解决什么问题
- 问题：
  - 序列长度受限：因成本与延迟，工业模型（包括TransAct V1）多处理短期序列（约100条），无法建模完整的用户生命周期行为（可能长达万条，丢失了蕴含丰富长期兴趣的绝大部分历史数据。而 V1 里面依赖离线压缩，存在信息损失或成本高昂的问题。
  - 建模目标单一：主流CTR模型仅预测“点击”，缺乏对用户行为内在演变规律的显式建模，未与更深层的用户行为预测（如下一步动作）联合优化，限制了理解深度。也就是打破了 V1 对于时间顺序不重要的看法，需要学习行为之间的顺序性。
  - 工程方案低效：处理长序列的现有方案（如压缩、离线缓存）存在信息损失或成本高昂的问题。同时一般学术界常忽略将超长序列模型投入生产所面临的巨大存储、网络传输和计算成本。直接处理万级序列会导致存储成本O(L)和网络成本O(NL)不可承受。
- 方法：
  - 对所有类型序列，𝑺𝐿𝐿，𝑺𝑅𝑇， 𝑺𝑖𝑚𝑝，都采用了 NN search 的方式，有效的压缩了实际进模型的序列长度
  - 提出了辅助任务，利用对比学习，学习下一个action 和 负样本直接的关系
  - 提出了一系列的工程优化

## 为啥这个新东西会有效，有什么优势
- 对比大部分序列模型
  - TransAct本身的优势同 V1 一样
  - 终身信息的充分利用，通过动态检索，模型能为每个候选物品“回顾”用户整个生命周期的相关兴趣，推荐决策的信息基础极大丰富。
  - 下一动作预测任务作为一种强有力的正则化，引导模型学习真实的用户行为模式，使学到的表征不仅能反映“用户喜欢什么”，还能理解“用户兴趣如何变化”，从而做出更精准、更多样的推荐。
- 工程对比：
  - 通过 “NN特征记录” 将训练数据存储开销从O(L)降至O(1)
  - 通过 “广播序列+实时NN搜索” 避免了O(NL)的网络传输
  - 通过 “定制Triton内核” 实现算子深度融合，推理比PyTorch快6.6倍
  - 等一系列工程优化，让 LL sequence 变的可行在实际生产环境中

## 与这个新东西类似的东西还有啥，相关的思路和模型
- 基于用户行为的序列的模型：
  - BST，同样采用了 Transformer，但是BST没有长期行为 sequence
  - TWIN / SIM，同属“两阶段”范式（先检索，后编码）。核心区别在于，TWIN等采用离线、静态的压缩或检索（如聚类），其检索过程与当前候选物品无关，可能导致信息损失。而TransAct V2是在线、动态、面向候选的实时检索，保留了最相关、最完整的信息。
  - TransAct V1，是TransAct V2的直接前身。核心区别在于，V1采用“实时模块+批量模块”的混合架构来近似长短期兴趣，但批量模块是离线计算的静态嵌入。没有实现端到端的训练。
  - 其它短期行为序列模型（DIEN 之类），没有采用混合行为序列，更多的是单一行为。
- 辅助任务引入：
  - 和 DIEN 一样都有辅助任务，区别在于 DIEN 是 GRU，DIEN loss 是 CE 不是对比学习。对比学习学到的负样本的信息更多更丰富。

## 在工业上通常会怎么用，如何实际应用
- 数据：
  - sequence 里面的行为参考 V1 都可以试一试，这里sequence的构建是根据先根据行为再根据 item 来排序的，也就是说单个 item 可以有多个行为在同一个位置上，不会展开重复 item
  - 辅助任务的负样本选择非常值得试一试，可以参考曝光但是用户没有点击的同一个 request 里面的其它 item，但是正样本的选择很难，因为搜索是明显有意图的 session
- 短期：
  - TransAct V2：
    - 同 V1 整体一样，从特征拼接，到 early fusion，到 transformer 的结构，到输出的压缩都可以参考
- 长期：
  - 可以试一试扩展到 LL sequence，对所有类型的 sequence 都采用 NN search，但是这样 item 就必须采用离线训练好的embedding
- 辅助任务：
  - 可以参考如何加入进去，特别是正样本的选择，负样本可以完全参考同样思路
- 工程：
  - 所有优化均可以尝试

## 参考
- https://zhuanlan.zhihu.com/p/1968013919038701871



