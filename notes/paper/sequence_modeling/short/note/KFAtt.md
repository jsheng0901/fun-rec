# Kalman Filtering Attention for User Behavior Modeling in CTR Prediction

# 标题
- 参考论文：Kalman Filtering Attention for User Behavior Modeling in CTR Prediction
- 公司：JD
- 链接：https://arxiv.org/pdf/2010.00985
- Code：https://github.com/lucidrains/kalman-filtering-attention
- 时间：2020
- `泛读`

# 内容

## 摘要
- 问题：
  - 传统注意力大多将关注范围仅限于单个用户的行为历史，这不适用于电商场景，因为用户常常会寻找与任何历史行为都无关的新需求。
  - 其次，这些注意力通常偏向于高频行为，而高频并不一定意味着重要性高。
- 方法：
  - 提出卡尔曼滤波注意力（KFAtt）。
    - 它将注意力中的加权池化操作视为一个最大后验（MAP）估计问题。
    - 引入先验信息，KFAtt能够在用户相关行为很少时，借助于全局统计信息。解决问题1。
    - 该机制还融入了频率限制机制，以纠正对高频行为的偏差。解决问题2。

## 1 INTRODUCTION
- 问题：
  - 目前通用的方式：通过为历史行为分配权重并进行加权池化来估计用户的潜在兴趣。这些权重通过各种注意力机制计算，以强调与查询相关的行为，抑制无关行为
    - 受限的关注范围：
      - 传统注意力大多假设，用户在当前查询词下的兴趣必定包含在其历史行为中。
      - 这个假设在电商场景中通常不成立，因为用户常常会寻找与任何历史行为都无关的新需求。
      - 在这种情况下，无论权重如何分配，仅关注历史行为，大多会偏离用户的真实兴趣，从而误导CTR预测系统。
      - **本质上是电商环境下，兴趣迁移很正常，仅聚焦单用户历史行为，无法应对用户寻找与历史无关的新需求的场景**。
    - 对高频行为的偏见：
      - 传统注意力将历史行为视为彼此独立，而忽略了行为与其对应查询之间的层次关系。
      - 同一查询下的行为高度同质，但在加权池化中却重复贡献。这必然使注意力权重偏向高频查询，而高频并不一定意味着重要性高。
      - 同时查询本身频率的巨大差异，这种偏见更为严重。一个无关但高频的查询很容易压制一个密切相关但低频的查询，最终降低CTR预测的准确性。
      - **本质上是过去的高频购买查询行为会占据主导当计算注意力的时候，但是可能与当前的候选item并没有关系，特别是复购率底的物品**
- 方法：
  - KFAtt-base：
    - 解决第一个问题
    - 历史行为可被建模为对隐藏用户兴趣的“测量值”，每个值具有不同程度的“不确定性”。将隐藏用户兴趣的估计形式化为最大后验（MAP）问题，并给出了一个简单而有效的闭式解。
    - 相比传统注意力，该解包含了一个额外的全局先验，使得即使在相关历史行为很少时，也能进行无偏的潜在兴趣预测。
  - KFAtt-freq：
    - 解决第二个问题
    - KFAtt-freq将每个去重后的查询建模为一个“传感器”，而将该查询下的行为视为来自该传感器的“重复测量”。
    - 相比传统注意力，该解决方案限制了同一查询下行为的总权重，从而纠正了针对高频查询的注意力偏见。
- **主要贡献**：
  - 指出在电商环境下的局限：
    - 对单个用户行为关注范围有限，以及对高频行为的注意力偏见。
  - 提出了一种新型的注意力机制KFAtt，它成功解决了上述两个局限。
  - 基于KFAtt，提出了一种满足严格在线延迟要求的高效行为建模模块，并上线部署。

## 2 Related Work
- CTR模型的三阶段演进：
  - 早期：线性、协同过滤、树模型。
  - 深度学习期：Embedding & MLP 成为主流范式。
  - 深度学习后期：引入多项式网络等进行特征交互。
- 用户行为建模的两个方向：
  - 目标无关建模：提取用户通用兴趣，比如离线计算通用用户兴趣，效率高但精准度有限。
  - 目标相关建模：使用注意力机制，提取针对特定候选商品的兴趣，效果显著，已成为主流。
- 现有研究的局限与KFAtt的定位：
  - 局限：当前研究多聚焦于将注意力机制与不同网络结构结合（RNN、记忆网络、Transformer），却忽视了对注意力机制本身固有缺陷（如视野狭窄、频率偏见）的深入探讨。
  - 定位：KFAtt论文的核心贡献正是直接针对注意力机制自身的两大根本局限提出创新解决方案，而非仅仅应用注意力。
- 与外部工作的区别：
  - 相比非CTR领域仅描述“不确定性”的注意力研究，KFAtt通过贝叶斯先验做出了更进一步的无偏估计。
  - 相比NLP领域提出解决全局频率偏见（逆词频来解决对高频行为的偏见）的方法，KFAtt专注于解决单个用户行为序列内部的频率偏差问题。以防止注意力输出被无关但高频的行为所主导。

## 3 Method

### 3.1 Preliminaries
- CTR输入构成: 
  - CTR = f(query,user behaviors,user profile,item profile,contexts)
- 行为建模的核心目标:
  - 从历史行为中提取出用户的隐性兴趣（Hidden Interest）
- 当前注意力机制的通用范式:
  - 使用注意力机制进行加权池化
- 问题：
  - 注意力场受限：
    - 仅依赖单用户历史，无法处理用户产生与历史无关的新需求的情况。
  - 频率偏见：
    - 权重计算会向高频出现的行为倾斜，即便这些行为与当前需求并不相关

### 3.2 Kalman Filtering Attention for User Behavior Modeling
- 核心：
  - 将“用户兴趣”视为待估状态：
    - 将当前查询 q 下的真实用户兴趣 v_q 建模为一个服从高斯分布的隐变量 v_q ∼ N(μ_q, σ_q²I)
  - 将“历史点击”视为带噪声的观测：
    - 每个历史点击 v_t 被视作对隐变量 v_q 的一次“测量”，测量噪声（不确定性）σ_t 取决于历史查询 k_t 与当前查询 q 的相关性（相关性越低，噪声越大）。
    - **本质上就是，和当前query相关性低的，当前历史时刻对应的行为带来的噪音就越多**
  - 引入“全局先验”作为校准：
    - μ_q 代表全体用户在相同查询 q 下的平均兴趣，当某个用户对当前查询没有相关历史时，模型参考其他用户在该查询下的普遍选择。
    - σ_q 代表该查询下兴趣的多样性（查询的固有属性）。比如广泛的搜索词（如“礼物”）具有较高的不确定性，而精确的词（如特定品牌名）则不确定性较低。
- 方法：
  - 最大化后验概率（MAP） 来估计 v_q，得到了一个优雅的闭式解：vˆ_q = ( (1/σ_q²) * μ_q + Σ (1/σ_t²) * v_t ) / ( 1/σ_q² + Σ (1/σ_t²) )
  - 全局先验兴趣与所有历史行为观测值的加权平均。权重是各自精度（方差的倒数） 的权衡。
- 优点：
  - 优势：解决“零相关历史”
    - 当用户当前需求在自身历史中无相关行为时（所有 σ_t 都很大），传统注意力失效。而KFAtt的公式中，全局先验项 (1/σ_q²) * μ_q 将成为主导，使估计结果合理地偏向大多数用户的共同兴趣，实现了“无相关行为时，借助群体智慧”。
  - 理论优势：兼容并泛化传统注意力
    - 当设置 σ_q = ∞（即忽略全局先验）且 1/σ_t² = exp(q^T k_t) 时，KFAtt的解完全退化为传统注意力形式。这证明传统注意力是KFAtt的一个特例，KFAtt是一个更具一般性的理论框架。
  - 工程优势：即插即用的灵活性
    - 公式中的 1/σ_t² 可以直接替换为任何现有注意力机制（如DIN、DSIN）计算出的当前候选 item/query 和每一个时刻历史行为的权重。这意味着KFAtt能作为一种即插即用的增强模块，无缝提升现有模型的性能。

### 3.3 Kalman Filtering Attention with Frequency Capping
- 核心：
  - KFAtt-base：将每个历史点击行为视为一个独立的“传感器”测量。
  - KFAtt-freq：将每个去重后的历史查询视为一个“传感器”，而将该查询下的所有点击行为，视为来自同一个传感器的多次重复观测。而不是多个新传感器的输入。
- 方法：
  - 对观测误差进行了更符合现实的分解，分为两部分：
    - 系统误差 σ_m：由传感器（查询 k_m）与待测目标（当前查询 q）的距离（不相关性） 决定。相关性越低，σ_m 越大。
    - 随机误差 σ‘_m：同一传感器（同一查询）进行多次观测时固有的随机波动。
    - 通过这种分解，模型能够识别出：即使观测次数再多，如果传感器本身（查询词）离目标很远，其提供的信息价值也是有上限的。
    - 得到一个新的最大化后验概率：vˆ_q = ( (1/σ_q²) * μ_q + Σ [ 1/(σ_m² + σ‘_m²/n_m) ] * v̄_m ) / ( 1/σ_q² + Σ [ 1/(σ_m² + σ‘_m²/n_m) ] )
    - v̄_m 是查询 k_m 下所有点击的平均向量，也就是传感器 k_m 上所有观测值的均值，n_m 是该查询下的点击次数
  - 如何限制高频不相关查询的权重，也就是频率封顶：
    - 公式中的权重项 1/(σ_m² + σ‘_m²/n_m)，总误差方差 = 系统误差方差 σ_m² + 随机误差方差 σ‘_m² 除以观测次数 n_m。
    - 原理：
      - 当一个查询 k_m 与当前需求高度不相关（即系统误差 σ_m 很大）时，无论这个查询被重复点击了多少次（n_m 很大），σ_m² 主导分母，使得整个权重项仍然很小。这意味着无关高频查询无法主导结果。
      - 反之，对于一个高度相关（σ_m 很小）的低频查询，其权重仍能得到有效体现。
    - 特性：
      - 当 n_m → ∞ 时，也就是点击次数很多的时候，权重趋近于 1/σ_m²，而不会无限增大，在数学上实现了对同一查询总权重的“封顶”。
    - **本质上就是，把无论多点击多少次但是不相关的历史行为，控制在一个固定的权重内，这样就不会影响相关性行为的权重分配**

### 3.4 Kalman Filtering Attention in Real Online System
- KFAtt-trans：
  - Transformer 的编码器（Encoder），用于捕捉行为的相关性和动态行为
  - 以及一个基于 KFAtt 的解码器（Decoder），用于预测基于query下的特定查询的用户兴趣

#### 3.4.1 Encoder: Within Session Interest Extractor
- 输入准备 - 注入位置信息：
  - 在历史查询 k1:T 和点击 v1:T 的向量中加入 位置编码，使模型能够感知行为的先后顺序。这里序列有两种。
- 结构划分 - 按会话切分：
  - 根据行为发生的时间间隔，将长序列划分为多个会话。这是基于“会话内行为相关性强，会话间相关性弱”的观察，同DSIN等模型的核心思想一样。
- 核心计算 - 局部多头自注意力：
  - 为了平衡效果与效率，只在每个会话内部应用多头自注意力机制。
  - 公式 MultiHead(Ks, Ks, Vs) 表示对会话 s 内的所有行为，计算键、查询、值均来自自身，从而让每个行为都能与会话内所有其他行为进行交互，捕捉局部共现、互补或演化模式。
- 输出生成 - 精炼后的行为表示：
  - 自注意力的输出再经过一个全连接层，最终生成该会话的精炼后兴趣表示矩阵 Hs。
  - Hs 的每一行对应一个经过上下文信息增强的行为表示，它将作为下游KFAtt层的输入。
  - Hs ∈ R T_s × d_model，T_s 是 session s 内行为的个数

#### 3.4.2 Decoder: Query-specific Interest Aggregator
- 输入：
  - 接收由前session interest extractor 的输出、经过精炼的全局行为表示 K 和 H。
- 核心计算：
  - vˆq = Concat(head1, . . . , headh)WO
  - headi = KFAtt(q * WQi, K * WKi, H * WVi)
  - 其本质是以当前查询 q 为目标，对上面的全局兴趣提取结果进行一次兴趣汇聚。
- 输出：
  - 生成最终与当前查询高度相关的用户兴趣向量 vˆq，用于下游CTR预测。
- 适配方法：
  - 将原模型注意力计算出的相关性分数（如传统的 exp(q^T k)），直接赋给KFAtt框架中的系统误差精度 1/σ²。
- 实现：
  - 在公式(12)的多头设置中，只需令 1/σ_t² = exp(q^T W_i^Q W_i^{K^T} k)，即可将传统注意力头无缝替换为KFAtt头。
  - **本质就把多头计算出来的注意力，替换掉系统误差，然后放进公式就可以计算出 KFAtt 的每个行为的新的兴趣影向量了**。

### 3.5 Why Kalman Filtering?
- 理论：
  - 在用户行为建模中，每一个历史行为都可以被视为是对当前兴趣的一次测量，通过 KF 框架，可以将这些分散的、带噪声的行为数据有效地“融合”成一个准确的兴趣估计值
- 数学：
  - 传统注意力机制通常被视为一种简单的加权平均（Weighted Sum）。
  - KFAtt 将其提升到了**最大后验概率估计（MAP）**的高度。这意味着模型不仅看用户“做了什么”（测量值），还参考了该查询下大众普遍的偏好“是什么”（先验知识）
- 解决“冷启动”问题：
  - 传统注意力机制完全依赖用户的历史记录。如果一个用户搜索了一个全新的领域（即冷启动场景），其历史记录中没有相关行为，传统模型就会失效。
  - 由于 KFAtt 引入了基于卡尔曼滤波的先验项 (μ_q)，即使在缺乏个人历史数据时，模型也能利用全局统计信息给出合理的初始预测。

### 4 Experiments

#### 4.1 Dataset and Evaluation Metrics
- Amazon Dataset
- JD：
  - 10B train first 32 days
  - next day 0.5B test
  - 70 days in sequence behaviors
- Metric:
  - AUC

#### 4.2 Compared Algorithms
- Pooling: sum
- Vanilla Attention
- DIN
- DIEN

#### 4.3 Implementation Details
- 96 features
- 16 embedding dims
- 最终 MLP - [1024, 512, 256, 1]
- 30 minutes sessions
- 10 sessions, 25 behaviors inside each session
- vˆq 64 维

#### 4.4 Comparison with State-of-the-arts
- KFAtt-trans-f 最好

#### 4.5 Adaptation to Various Attentions Mechanisms
- 用任意attention替换，再 apply KFAtt-trans-f，都可以大幅提升效果。

#### 4.6 Experiments on Real Production Dataset & Online A/B Testing
- 4.4% CTR
- 4.06% eCPM
- latency T99, better than DIEN, almost same as DIN, Transformer

### 5 Conclusions
- 核心：
  - KFAtt 的本质创新在于将注意力机制从经验性的加权计算提升到了统计学上的最大后验概率（MAP）估计
- 未来：
  - 适用于其它场景，比如推荐系统
  - 更多 attention 机制 (e.g. self-attention, co-attention)
  - 更多数据类型 (e.g. sequence, image and graph)
  - 区间估计。传统的注意力机制只给出一个点估计的结果，而基于 KF 的框架未来可以给出预测的置信度，适用于需要高可靠性的决策场景。


# 思考

## 本篇论文核心是讲了个啥东西
- 提出了KFAtt，一个基于贝叶斯状态估计理论的注意力框架。把问题定义成在给定一系列带噪声的“行为观测值”和一个“全局兴趣先验”的条件下，对用户当前隐藏兴趣进行最大后验概率估计的问题。
- KFAtt-base：解决注意力视野局限于单用户历史的问题。引入先验信息，能够在用户相关行为很少时，借助于全局统计信息。
- KFAtt-freq：进一步解决注意力权重偏向高频行为的问题。
- KFAtt-trans：系统化的把transformer融入进来


## 是为啥会提出这么个东西，为了解决什么问题
- 问题：
  - 视野狭窄：传统注意力（如DIN）只“看”用户自己的历史行为。当用户产生全新需求（如第一次搜索“订婚礼物”）时，由于缺乏相关历史，模型无法做出有效预测。
  - 频率偏见：传统注意力将每个行为独立计算权重，导致同一高频查询下的重复点击会过度主导兴趣表示，但高频不等于高相关性，这会淹没低频但强相关的信号。
- 方法：
  - KFAtt-base，同上
  - KFAtt-freq，同上

## 为啥这个新东西会有效，有什么优势
- 理论优势：
  - 提供了一个统一且可解释的数学框架。传统注意力被证明是KFAtt在忽略全局先验时的特例。
- 性能优势：
  - 引入全局先验 μ_q：
    - 当用户自身历史不相关时，模型能自动借助全体用户的共性（μ_q）进行合理的兴趣预测，解决了“零相关历史”的冷启动问题。
  - 频率封顶机制（Frequency Capping）：
    - 在KFAtt-freq 中，模型将行为误差分解为系统误差（查询相关性）和随机误差（多次观测波动）。这使得同一查询下的行为总权重被“封顶”，防止高频无关行为干扰预测
- 工程优势：
  - 即插即用。其系统误差项 1/σ² 可直接套用现有注意力（如DIN、Transformer）的权重，能无缝增强现有工业模型，升级成本低。

## 与此论文类似的东西还有啥，相关的思路和模型
- 对比DIN/DIEN：
  - KFAtt是通用版本，KFAtt的理论框架包含了它们，并解决了它们固有的视野和频率偏差问题。
- 对比DSIN：
  - 同样都是基于session进行提取兴趣先
  - 这些模型可作为KFAtt的前端编码器，负责处理长序列；KFAtt则作为后端解码器，负责兴趣提取与偏差修正。

## 论文有什么可以改进的地方，可以后续继续拓展研究
- 先验信息的深度挖掘：
  - 当前全局先验 μ_q 较简单。可探索更复杂的先验，如分人群、分场景的先验，或引入知识图谱来构建更丰富的语义先验。
  - 或者加入时间decay的概念进先检信息
- 与序列建模模块的端到端优化：
  - 论文将会话Transformer和KFAtt解码器相对分离。未来可探索更紧密的联合优化架构，让序列编码直接服务于最优的贝叶斯估计。
- 应用于更广泛的偏差问题：
  - KFAtt的贝叶斯框架有望被迁移以解决推荐系统中的其他偏差，如曝光偏差、位置偏差等。
- 高效部署的进一步优化：
  - 尽管已做会话局部优化，但对于超长生命周期的用户，如何更低成本地动态更新和维护其兴趣“记忆”，仍有探索空间。
  - 也就是如何扩展序列长度到lifelong的应用

## 在工业上通常会怎么用，如何实际应用
- KFAtt-base 完全可以照搬，用不同的attention试一试
- KFAtt-freq 需要分析用户行为是否有这个特性再使用
- KFAtt-trans 的编码和解码两步都可以试一试

## 参考
- https://zhuanlan.zhihu.com/p/266298467
- https://zhuanlan.zhihu.com/p/413134465


